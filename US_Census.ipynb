{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "US Census.ipynb",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPUs6dMRnqD6CN5DX4um8Ze",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/btrentini/data_science/blob/master/US_Census.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81CfsK3CSLmL",
        "colab_type": "text"
      },
      "source": [
        "#US Census \n",
        "\n",
        "‚ö† **Run notebook:** In the menu above click \"Runtime > Run all\" or press $CTRL + F9$ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t885lixZdVUS",
        "colab_type": "text"
      },
      "source": [
        "## Task Summary\n",
        "\n",
        "In summary, this is what this notebook does:\n",
        "\n",
        "- Extracts and Prepares dataset from http://thomasdata.s3.amazonaws.com/ds/us_census_full.zip\n",
        "- EDA and Feature Engineering\n",
        "- Learns, for a dataset formed by vectors $\\{x_n\\}_{n=0}^N$, N the number of training examples, a model for the label $y_n$.  \n",
        "- Predicts, given an a vector $x_n=\\{x_n^0, x_n^1, ..., x_n^k\\}$, $k$ the number of features, and the model learnt, if target variable, $y$ (salary), is greater than or equal to \\$50,000 per year, yielding a prediction $\\hat{y_n}$ for each training example $x_n$.\n",
        "- Test different models and validate on test set the cross-entropy loss $L = - \\sum_n y_n \\, \\,log \\, \\hat{y_n} + (1- y_n)log(1-\\hat{y_n})$ that yields the lowerst error for the pair $(y, \\hat{y})$ \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oNlPo0Xdc9w",
        "colab_type": "text"
      },
      "source": [
        "## Task Info\n",
        "\n",
        "The following link lets you download an archive containing an ‚Äúexercise‚Äù US Census dataset: http://thomasdata.s3.amazonaws.com/ds/us_census_full.zip\n",
        "This US Census dataset contains detailed but anonymized information for approximately 300,000 people.\n",
        "\n",
        ">The archive contains 3 files: \n",
        "* A large training file (csv)\n",
        "* Another test file (csv)\n",
        "* A metadata file (txt) describing the columns of the two csv files (identical for both)\n",
        "\n",
        "> **The goal** of this exercise is to model the information contained in the last column (42nd), i.e., whether a person makes more or less than $50,000 per year, from the information contained in the other columns. The exercise here consists of modeling a binary variable.\n",
        "\n",
        "> Work with Python (or R) to carry out the following steps:\n",
        "*  Load the train and test files.\n",
        "* Perform an exploratory analysis on the data and create some relevant visualisations.\n",
        "* Clean, preprocess, and engineer features in the training data, with the aim of building a data set that a model will perform well on.\n",
        "* Create a model using these features to predict whether a person earns more or less than $50,000 per year. Here, the idea is for you to test a few different models, and see whether there are any techniques you can apply to improve performance over your first results.\n",
        "* Choose the model that appears to have the highest performance based on a comparison between reality (the 42nd variable) and the model‚Äôs prediction. \n",
        "* Apply your model to the test file and measure its real performance on it (same method as above).\n",
        "\n",
        ">The goal of this exercise is not to create the best or the purest model, but rather to describe the steps you took to accomplish it.\n",
        "Explain areas that may have been the most challenging for you.\n",
        ">Find clear insights on the profiles of the people that make more than $50,000 / year. For example, which variables seem to be the most correlated with this phenomenon?\n",
        ">Finally, you push your code on GitHub to share it with me, or send it via email.\n",
        "\n",
        ">Once again, the goal of this exercise is not to solve this problem, but rather to spend a few hours on it and to thoroughly explain your approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2mD8YwKdgTW",
        "colab_type": "text"
      },
      "source": [
        "## Metadata Info\n",
        "\n",
        "**From the metadata (see below how this was obtained):**\n",
        "\n",
        "\n",
        "> This data was extracted from the census bureau database found at\n",
        ">http://www.census.gov/ftp/pub/DES/www/welcome.html\n",
        "\n",
        ">Donor: Terran Lane and Ronny Kohavi\n",
        "       Data Mining and Visualization\n",
        "       Silicon Graphics.\n",
        "       e-mail: terran@ecn.purdue.edu, ronnyk@sgi.com for questions.\n",
        "\n",
        "\n",
        ">The data was split into train/test in approximately $2/3$, $1/3$ proportions using MineSet's MIndUtil mineset-to-mlc.\n",
        "\n",
        ">**Prediction task** is to determine the income level for the person represented by the record.  Incomes have been binned at the $50K level to present a binary classification problem, much like the original UCI/ADULT database.  The goal field of this data, however, was drawn from the \"total person income\" field rather than the \"adjusted gross income\" and may, therefore, behave differently than the orginal ADULT goal field.\n",
        ">More information detailing the meaning of the attributes can be found in http://www.bls.census.gov/cps/cpsmain.htm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OBKRcB2nsAy",
        "colab_type": "text"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWzMU7dj5UIT",
        "colab_type": "text"
      },
      "source": [
        "## Check GPU configs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7LH86If5PJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzfveUw-5RfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvcc --version"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVW8D1DcT8n3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fuuDAjt5b8Q",
        "colab_type": "text"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ln5s1-7OVfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# System utils\n",
        "import sys\n",
        "import os\n",
        "import zipfile\n",
        "import time\n",
        "\n",
        "# Parallelism\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Some classic data science stuff\n",
        "import numpy as np; \n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt \n",
        "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
        "%matplotlib inline \n",
        "\n",
        "# Math & Stats\n",
        "import math \n",
        "import scipy.stats as stats\n",
        "from statsmodels.tools.tools import add_constant\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor    \n",
        "\n",
        "# Machine Learning libs\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier, plot_importance\n",
        "\n",
        "# Scikit learn utils\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNbDzwPuTelT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Styling\n",
        "sns.set_style(\"ticks\", {\"xtick.major.size\": 11, \"ytick.major.size\": 11})\n",
        "sns.set_palette(\"inferno\")\n",
        "sns.set(font_scale = 2)\n",
        "figsize=(23, 15)\n",
        "pal='inferno'\n",
        "div = \"=\"*72"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Hq-GXmVSwF7",
        "colab_type": "text"
      },
      "source": [
        "# Build datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPQJnPZGeQ8e",
        "colab_type": "text"
      },
      "source": [
        "## Download & Extract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX-F4BJ7S6XX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget \"http://thomasdata.s3.amazonaws.com/ds/us_census_full.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV2CYp6XdZoX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsmLt9msRGhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define helper to load\n",
        "local_zip =os.path.join('/content', 'us_census_full.zip')\n",
        "\n",
        "# Unzip Train Set into temporary path\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZbPCGafRVYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -1 /tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h1tILn2RlvU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train     = '/tmp/us_census_full/census_income_learn.csv'\n",
        "test      = '/tmp/us_census_full/census_income_test.csv'\n",
        "metadata  = '/tmp/us_census_full/census_income_metadata.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHoJp1ItR9gw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's see what's in the metadata\n",
        "!fold -w 130 -s $metadata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vtj3S3yWUOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check if there's a header in the train file\n",
        "!head -2 $train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGctKhCXWl0w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check if there's a header in the test file\n",
        "!head -2 $test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAYWLzTfR-WP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%time df  = pd.read_csv(train, header=None)\n",
        "%time t_df = pd.read_csv(test, header=None) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qoFDP9VVpVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TZZbcsDXId7",
        "colab_type": "text"
      },
      "source": [
        "## A trick from metadata for column names\n",
        " This will help us a lot during EDA. The metada contains useful information about columns, values and their properties. I can use this file to name columns and later on this will give us the option to address the dataframe by column names, which might be handy in many cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJB6tkeYYnHZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!tail -42 $metadata"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzCgdwxDXQOl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "From the above we can see that the last 42 rows are the column names\n",
        "We can use this info to improve our datasets and help us eith EDA\n",
        "\n",
        "Besides, the metada tell us to ignore '|_instance_weight'the 24th record\n",
        "'''\n",
        "\n",
        "# We will beed a list to append to...\n",
        "cols = []\n",
        "\n",
        "# Save metadata last 42 rows\n",
        "column_names = !tail -42 $metadata\n",
        "\n",
        "# Remove the record to be ignored '|_instance_weight'\n",
        "column_names.pop(24)        \n",
        "\n",
        "# Build column helper\n",
        "for col in column_names:\n",
        "  record = col.split(\":\")[0].replace(\" \",\"_\")\n",
        "  cols.append(record)\n",
        "\n",
        "# Add target variable's column not listed in metadata\n",
        "cols.append(\"target\")\n",
        "\n",
        "# Insert column names into dataframes\n",
        "df.columns = cols\n",
        "t_df.columns = cols\n",
        "\n",
        "# Voila!\n",
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvzpPKvlhpEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.target.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwqM_P2ojb6W",
        "colab_type": "text"
      },
      "source": [
        "**Note:** Dataset quite unbalanced...So we have two options:\n",
        "\n",
        "- Either do Downscaling / Upscaling; Or\n",
        "- Create a method that accounts for this imbalance in the proper way\n",
        "\n",
        "More on this later..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnFYlzaaygbU",
        "colab_type": "text"
      },
      "source": [
        "**Transform year columns** as its obvious from Metadata it shouldn't be continuous"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idXHb_c63OV5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYNFQUMbwghU",
        "colab_type": "text"
      },
      "source": [
        "## Categorise data \n",
        "\n",
        "Lets convert objects into categorical values so that we can manipulate them numerically"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3at83Tc69aX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obj = df.select_dtypes(['object'])\n",
        "df[obj.columns] = obj.apply(lambda x: x.str.strip().replace(\" \",\"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_DGRLCJzaG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_bkp = df.copy()\n",
        "t_df = t_df.copy()\n",
        "\n",
        "for col in df.columns:\n",
        "  if df[col].dtype == 'object':\n",
        "    df[col] = df[col].str.strip().replace(\" \",\"\").astype('category').cat.codes\n",
        "    t_df[col] = df[col].str.strip().replace(\" \",\"\").astype('category').cat.codes\n",
        "\n",
        "df.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCWb4udrsHMF",
        "colab_type": "text"
      },
      "source": [
        "## Are there null / undefined values?\n",
        "\n",
        "There are a few records marked with \"$?$\" but we will treat this as a category... So I wonder if there are columns with null values or \"na\" values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ni_f7tiH40W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ej-RSZOiHhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "null_columns=df.columns[df.isnull().any()]\n",
        "if len(null_columns) > 0:\n",
        "  print(df[null_columns].isnull().sum())\n",
        "else:\n",
        "  print(\"No null columns\")\n",
        "\n",
        "na_columns=df.columns[df.isna().any()]\n",
        "if len(na_columns) > 0:\n",
        "  print(df[na_columns].isna().sum())\n",
        "else:\n",
        "  print(\"No NA columns\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN2HNhaoXR40",
        "colab_type": "text"
      },
      "source": [
        "## Are there duplicates?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UkoaD4elXXkJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if df.duplicated().sum() > 0:\n",
        "  print(df.duplicated().sum(), \" dupicated records found and dropped\")\n",
        "  df.drop_duplicates(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qrmdLlOUag26",
        "colab_type": "text"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIco2eMgCTy",
        "colab_type": "text"
      },
      "source": [
        "## Feature Importance\n",
        "\n",
        "Start with a simple light GBM for estimating feature importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlhxcRqehvxE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LGBMClassifier(learning_rate=0.01, num_leaves= 33, random_state=42)\n",
        "model.fit(df.iloc[:,0:41], df.iloc[:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z6ARSQ9g2uc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(figsize))\n",
        "ax = sns.barplot(y=model.feature_importances_, x=df.iloc[:,0:41].columns, palette=pal)\n",
        "plt.xticks(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp4-7mwuoDlk",
        "colab_type": "text"
      },
      "source": [
        "## Correlation\n",
        "\n",
        "Will help us understand risks of colinearity and some features that we can get scrap. We will only consider features with moderate (>0.5), strong (>0.7) and very strong (>0.9) correlation in absolute values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9sWWoJGgA5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correlation = df.drop(['target'], axis=1).corr()\n",
        "mask = np.triu(correlation)\n",
        "with sns.axes_style(\"white\"):\n",
        "  f, ax = plt.subplots(figsize=(23,23))\n",
        "  #ax = sns.heatmap(correlation[(correlation > 0.5) | (correlation < -0.5)],\n",
        "  ax = sns.heatmap(correlation,\n",
        "              square=True,\n",
        "              vmax=1.0,\n",
        "              vmin=-1.0,\n",
        "              center=0.0,\n",
        "              cmap=\"coolwarm\",\n",
        "              linecolor='white',\n",
        "              linestyle = '--',\n",
        "              rasterized=False,\n",
        "              edgecolor='white',\n",
        "              capstyle='projecting',\n",
        "              linewidth=2,\n",
        "              mask=mask,\n",
        "              annot=False, \n",
        "              fmt=\".2f\",\n",
        "              robust=True,\n",
        "              cbar=True,\n",
        "              cbar_kws={\"location\": \"top\",\n",
        "                        'use_gridspec': False,\n",
        "                        \"label\": \"Correlation Coefficient\",\n",
        "                        'shrink': 0.8})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvq-SgnmYkbN",
        "colab_type": "text"
      },
      "source": [
        "### Variance Inflation Factor(VIF)\n",
        "From wikipedia: VIF is the quotient of the variance in a model with multiple terms by the variance of a model with one term alone. It quantifies the severity of multicollinearity in an ordinary least squares regression analysis. It provides an index that measures how much the variance (the square of the estimate's standard deviation) of an estimated regression coefficient is increased because of collinearity.\n",
        "\n",
        "> Multicollinearity is a common phenomenon in high‚Äêdimensional settings, in which two or more predictor variables are highly correlated [Zhao et al, 2020]( https://doi.org/10.1002/sta4.272)\n",
        "\n",
        "This is a bit of a controversial topic and \"tule of thumb\" tresholds are dangerous as pointed in [A Caution Regarding Rules of Thumb\n",
        "for Variance Inflation Factors](https://www.researchgate.net/profile/Robert_Obrien8/publication/226005307_A_Caution_Regarding_Rules_of_Thumb_for_Variance_Inflation_Factors/links/54d0f2620cf298d656695641/A-Caution-Regarding-Rules-of-Thumb-for-Variance-Inflation-Factors.pdf). The lowest VIF valuee is 1. Anything beyond 10 is extreme. Most people choose either $3$, $4$ or $5$ as treshold. We will go with $4$.\n",
        "\n",
        "‚ùå We have $k=41$ features. With large $k$ this would be innapropriate but there are solusions like the one proposed in [Zhao et al, 2020]( https://doi.org/10.1002/sta4.272)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0lMU06H6YFJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = add_constant(df.iloc[:,0:41])\n",
        "multico_indx=pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOLDo3ix-xcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.distplot(multico_indx, rug=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhyaWPMJfTuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.iloc[:,np.where((multico_indx[1:] < 4)==True)[0][:].tolist()].dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DDSZXe3GzWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df.iloc[:,np.where((multico_indx[1:] < 4)==True)[0][:].tolist()]\n",
        "df['target'] = df_bkp['target'].astype('category').cat.codes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRmDcR1kHgkX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIU2HOZSqUOp",
        "colab_type": "text"
      },
      "source": [
        "### Post VIF check feature importance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lsf0R6GuqYTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = LGBMClassifier(learning_rate=0.01, num_leaves= 33, random_state=42)\n",
        "model.fit(df.iloc[:,:-1], df.iloc[:,-1])\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(figsize))\n",
        "ax = sns.barplot(y=model.feature_importances_, x=df.iloc[:,:-1].columns, palette=pal)\n",
        "plt.xticks(rotation=90)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viJ_sXsBbazN",
        "colab_type": "text"
      },
      "source": [
        "### Post VIF check correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgoWFDjZa5XQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correlation = df.drop(['target'], axis=1).corr()\n",
        "mask = np.triu(correlation)\n",
        "with sns.axes_style(\"white\"):\n",
        "  f, ax = plt.subplots(figsize=(23,23))\n",
        "  #ax = sns.heatmap(correlation[(correlation > 0.5) | (correlation < -0.5)],\n",
        "  ax = sns.heatmap(correlation,\n",
        "              square=True,\n",
        "              vmax=1.0,\n",
        "              vmin=-1.0,\n",
        "              center=0.0,\n",
        "              cmap=\"coolwarm\",\n",
        "              linecolor='white',\n",
        "              linestyle = '--',\n",
        "              rasterized=False,\n",
        "              edgecolor='white',\n",
        "              capstyle='projecting',\n",
        "              linewidth=2,\n",
        "              mask=mask,\n",
        "              annot=False, \n",
        "              fmt=\".2f\",\n",
        "              robust=True,\n",
        "              cbar=True,\n",
        "              cbar_kws={\"location\": \"top\",\n",
        "                        'use_gridspec': False,\n",
        "                        \"label\": \"Correlation Coefficient\",\n",
        "                        'shrink': 0.8})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjw4hlzgKvPB",
        "colab_type": "text"
      },
      "source": [
        "It Looks like country of birth from father and mother are moderately correlated with citizenship. Since we can (almost) always derive someone's citizenship from their parents' place of birth and assuming parents' nationality do not play an imediate factor in someone's wages (setting apart deeper social analyses) and considering neither parent's origin appears as a crucial feature in our Light GBM model, we will remove these features to simplify our model further"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW915kP1Je6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(columns=['country_of_birth_father'], inplace=True)\n",
        "df.drop(columns=['country_of_birth_mother'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXcuJKlhKJHK",
        "colab_type": "text"
      },
      "source": [
        "There's a moderate linear relationship between the industry of a person with its ocupation and the number of persons worked for employer. Althought these shouldn't affect our model aggressively, we can infer one by the other (for instance: we assume that someone's occupation is highly related to its industry and that its industry can tell information about the number of people per employer. Think about Social Media industry versus Manufacuring plant.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCp9dxWcJ-ky",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(columns=['detailed_occupation_recode'], inplace=True)\n",
        "df.drop(columns=['num_persons_worked_for_employer'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaO4q8ZtLBiT",
        "colab_type": "text"
      },
      "source": [
        "In the USA, similar to the UK and other countries like Switzerland, someone's marital status will change the tax code. So we can infer the former by the later and can get rid of another feature with moderate correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QksnpSBoLBJF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.drop(columns=['marital_stat'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLtCrvMKJu0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correlation = df.drop(['target'], axis=1).corr()\n",
        "mask = np.triu(correlation)\n",
        "with sns.axes_style(\"white\"):\n",
        "  f, ax = plt.subplots(figsize=(23,23))\n",
        "  ax = sns.heatmap(correlation[(correlation > 0.5) | (correlation < -0.5)],\n",
        "  #ax = sns.heatmap(correlation,\n",
        "              square=True,\n",
        "              vmax=1.0,\n",
        "              vmin=-1.0,\n",
        "              center=0.0,\n",
        "              cmap=\"coolwarm\",\n",
        "              linecolor='white',\n",
        "              linestyle = '--',\n",
        "              rasterized=False,\n",
        "              edgecolor='white',\n",
        "              capstyle='projecting',\n",
        "              linewidth=2,\n",
        "              mask=mask,\n",
        "              annot=False, \n",
        "              fmt=\".2f\",\n",
        "              robust=True,\n",
        "              cbar=True,\n",
        "              cbar_kws={\"location\": \"top\",\n",
        "                        'use_gridspec': False,\n",
        "                        \"label\": \"Correlation Coefficient\",\n",
        "                        'shrink': 0.8})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTGf-T1TNnxY",
        "colab_type": "text"
      },
      "source": [
        "**NICE!** *So no correlations greater than 0.5 or less than -0.5. Let's plot the rest:*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJEQQMOaNyiI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correlation = df.drop(['target'], axis=1).corr()\n",
        "mask = np.triu(correlation)\n",
        "with sns.axes_style(\"white\"):\n",
        "  f, ax = plt.subplots(figsize=(23,23))\n",
        "  #ax = sns.heatmap(correlation[(correlation > 0.5) | (correlation < -0.5)],\n",
        "  ax = sns.heatmap(correlation,\n",
        "              square=True,\n",
        "              vmax=1.0,\n",
        "              vmin=-1.0,\n",
        "              center=0.0,\n",
        "              cmap=\"coolwarm\",\n",
        "              linecolor='white',\n",
        "              linestyle = '--',\n",
        "              rasterized=False,\n",
        "              edgecolor='white',\n",
        "              capstyle='projecting',\n",
        "              linewidth=2,\n",
        "              mask=mask,\n",
        "              annot=True,\n",
        "              fmt=\".1f\",\n",
        "              robust=True,\n",
        "              cbar=True,\n",
        "              annot_kws={\"size\": 16},\n",
        "              cbar_kws={\"location\": \"top\",\n",
        "                        'use_gridspec': False,\n",
        "                        \"label\": \"Correlation Coefficient\",\n",
        "                        'shrink': 0.8})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnPXQk5OwUF2",
        "colab_type": "text"
      },
      "source": [
        "## Further EDA\n",
        "Let's explore our data a little further. Then perhaps come up with a simple hypothesis to test and a prediction model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r57JdQmObaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets use the non categorized dataset for some analyses\n",
        "eda = df_bkp.loc[:,df.columns.tolist()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VO_v-94wXN_",
        "colab_type": "text"
      },
      "source": [
        "## Distplot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEup2KZ-a1vE",
        "colab_type": "text"
      },
      "source": [
        "Warm up: do we have a population that is equally distirbuted in terms of age? No. We can see that 50% of the population is 30 years old or younger. We have the next 30% at circa 60 years old or less. We can also confirm that by fitting a loggama distribution to the data (second chart, black line)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOob7R0RrIGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Bimodal data, 50% of population less than 30yo\n",
        "fig, ax = plt.subplots(2, 1, figsize=figsize)\n",
        "kwargs = {'cumulative': True}\n",
        "\n",
        "g1 = sns.distplot(eda['age'], hist_kws=kwargs, kde_kws=kwargs, ax=ax[0], axlabel=None,\n",
        "                  fit=stats.gamma, bins=10)\n",
        "g2 = sns.distplot(eda['age'], rug=True, ax=ax[1], fit=stats.loggamma)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY2TxWHHYH_m",
        "colab_type": "text"
      },
      "source": [
        "How about the target by age? We can see that 50% of those earning $< 50$K are around 30 years old or young. On the other hand, 50% of those earning $>50$K are older than 45 years old."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3HHmKThUhFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(figsize))\n",
        "g = sns.boxplot(y='age', x='target', data=eda, palette=pal)\n",
        "plt.xticks(rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbZeEjDHcrS1",
        "colab_type": "text"
      },
      "source": [
        "# Hypothesis Test 1: Constructors vs Educators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8h8chOXcuEa",
        "colab_type": "text"
      },
      "source": [
        "**Let's distill the following hypothesis:**\n",
        "\n",
        "\"There's a better chance of seeing *Construction* workers among the top earners in comparison to *Education* workers. \n",
        "\n",
        ">$H_0: \\, \\, $ Equal chance of *Construction* workers earning  +50K as *Education* workers   ($\\theta=0.5$)    \n",
        ">$H_1: \\, \\, $ There's a better chance of seeing *Construction* workers earning  +50K in comparison to *Education* workers ($\\theta > 0.5$)  \n",
        "\n",
        "\n",
        "Where $H_0$ is our null hypothesis and $H_1$ is our alternative hypothesis, our test will be conducted at a significance level of 5%\n",
        "\n",
        "\n",
        "üöÄ Let's ignore EDA and answer this with the right level of confidence in a statistical way\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovqtVUOPDkpj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnNdR8JBfjRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's first compute the proportion of construction workers and educators among the top earners\n",
        "test_statistic = eda[(eda.major_industry_code==\"Construction\") & (eda.target!='-50000.') ].target.count()\n",
        "complement = eda[(eda.major_industry_code==\"Education\") & (eda.target!='-50000.') ].target.count()\n",
        "\n",
        "# How many observations do we have in total\n",
        "N = test_statistic+complement\n",
        "print(\"Total people earning +50K = \", N, test_statistic+complement==N)\n",
        "print(\"--- construction =\", test_statistic)\n",
        "print(\"--- education =\", complement)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_vlg6-Fi8XT",
        "colab_type": "text"
      },
      "source": [
        "**To simplify, we can model this as a binomial distribution** (seeing construction workers in the high earning group is considered success)\n",
        "\n",
        "*Level = 5%*:  Decreasing this will decrease our critical region (region where we fail\n",
        " o reject the null hypothesis if the p-value falls within)\n",
        "\n",
        "*p=0.5:*  We want to test if the propability of seeing constructors and educators are the same"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSlwiznwrjYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "level=0.05\n",
        "p=0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy8OKDaMjYAW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Within the 50K+ target we have seen {N} constructors and educators\")\n",
        "print(f\"{test_statistic} of them are in the Construction Sector ({round(test_statistic/N,2)*100}%)\\n\")\n",
        "print(f\"\\nH0 : The proportion of consutrction workers as top earners is the same as educators (param = {p})\")\n",
        "print(f\"H1 : There are fewer construction workers earning 50K+ than educators (param < {p})\\n\")\n",
        "\n",
        "%time p_value = np.sum([stats.binom_test(x, N, p=0.5, alternative='greater') for x in range(test_statistic)])\n",
        "print(f\"p-value = {p_value} -->  for seeing {test_statistic} construction workers or less\")\n",
        "\n",
        "print(\"\\n\", div)\n",
        "if p_value < level:\n",
        "  print('''\\nTHEREFORE We reject the null hypothesis H0.\n",
        "  There is enough evidence at 5% level of significance to suggest that \n",
        "  >> there are more constructors than educators in the top earners\\n''')\n",
        "else:\n",
        "  print('''\\nTHEREFORE We FAIL to reject the null hypotehesis H0.\n",
        "  There is enough evidence at 5% level of significance to suggest that \n",
        "  >> constructors and educators are balanced as top earners\\n''')\n",
        "print(div)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ9jUkGb3iyL",
        "colab_type": "text"
      },
      "source": [
        "# Hypothesis 2: What if we compare constructors against Agriculture workers? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0vBvSbF39yZ",
        "colab_type": "text"
      },
      "source": [
        "**Let's distill the following hypothesis:**\n",
        "\n",
        "\"There's a better chance of seeing *Construction* workers among the top earners in comparison to *Agriculture* workers. \n",
        "\n",
        ">$H_0: \\, \\, $ Equal chance of *Construction* workers earning  +50K as *Agriculture* workers   ($\\theta=0.5$)    \n",
        ">$H_1: \\, \\, $ There's a better chance of seeing *Construction* workers earning  +50K in comparison to *Agriculture* workers ($\\theta > 0.5$)  \n",
        "\n",
        "\n",
        "Where $H_0$ is our null hypothesis and $H_1$ is our alternative hypothesis, our test will be conducted at a significance level of 5%\n",
        "\n",
        "\n",
        "üöÄ Let's ignore EDA and answer this with the right level of confidence in a statistical way\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2ULm_3JCxVj",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGB5DDIU5hSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's first compute the proportion of construction workers and educators among the top earners\n",
        "test_statistic = eda[(eda.major_industry_code==\"Construction\") & (eda.target!=' -50000.') ].major_industry_code.value_counts()[0]\n",
        "complement = eda[(eda.major_industry_code==\"Agriculture\") & (eda.target!='-50000.') ].major_industry_code.value_counts()[0]\n",
        "\n",
        "# How many observations do we have in total\n",
        "N = test_statistic+complement\n",
        "print(\"Total people earning +50K = \", N, test_statistic+complement==N)\n",
        "print(\"--- construction =\", test_statistic)\n",
        "print(\"--- Agriculture =\", complement)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0OpBkDt5mbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "level=0.05\n",
        "p=0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y97OkBA25oU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f\"Within the 50K+ target we have seen {N} constructors and educators\")\n",
        "print(f\"{test_statistic} of them are in the Construction Sector ({round(test_statistic/N,2)*100}%)\\n\")\n",
        "print(f\"\\nH0 : The proportion of consutrction workers as top earners is the same as educators (param = {p})\")\n",
        "print(f\"H1 : There are fewer construction workers earning 50K+ than educators (param < {p})\\n\")\n",
        "\n",
        "%time p_value = np.sum([stats.binom_test(x, N, p=0.5, alternative='greater') for x in range(test_statistic)])\n",
        "print(f\"p-value = {p_value} -->  for seeing {test_statistic} construction workers or less\")\n",
        "\n",
        "print(\"\\n\", div)\n",
        "if p_value < level:\n",
        "  print('''\\nTHEREFORE We reject the null hypothesis H0.\n",
        "  There is enough evidence at 5% level of significance to suggest that \n",
        "  >> there are more constructors than educators in the top earners\\n''')\n",
        "else:\n",
        "  print('''\\nTHEREFORE We FAIL to reject the null hypotehesis H0.\n",
        "  There is enough evidence at 5% level of significance to suggest that \n",
        "  >> constructors and educators are balanced among top earners\\n''')\n",
        "print(div)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA-pfXBT1Ko4",
        "colab_type": "text"
      },
      "source": [
        "## Split Train into Train (70%) & Validation (30%)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IF05nk2S26Jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df # Is this the categorized one?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-CXifb6OTM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:,:-1]\n",
        "y = df.iloc[:,-1]\n",
        "\n",
        "xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05pT_ozRBiO4",
        "colab_type": "text"
      },
      "source": [
        "## XGBOOST\n",
        "Please note this is not the Scikit-learn API but the python API: https://xgboost.readthedocs.io/en/latest/python/python_intro.html which provides better support for GPUs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajZy3Fpw58mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# instantiate params\n",
        "params = {}\n",
        "\n",
        "# general params\n",
        "general_params = {'silent': 1,\n",
        "                  'lambda': 0.02,\n",
        "                  'learning_rate': 0.01,\n",
        "                  'max_depth': 7}\n",
        "params.update(general_params)\n",
        "\n",
        "# booster params\n",
        "n_gpus = 1  # change this to -1 to use all GPUs available or 0 to use the CPU\n",
        "booster_params = {}\n",
        "\n",
        "if n_gpus != 0:\n",
        "    booster_params['tree_method'] = 'gpu_hist'\n",
        "    booster_params['n_gpus'] = n_gpus   \n",
        "params.update(booster_params)\n",
        "\n",
        "# learning task params\n",
        "learning_task_params = {}\n",
        "learning_task_params['eval_metric'] = ['logloss', 'rmse']\n",
        "learning_task_params['objective'] = 'reg:logistic'\n",
        "\n",
        "params.update(learning_task_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn66QJG0Fdv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = XGBClassifier(**params, n_estimators=1000, early_stopping_rounds=10, verbose=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc8lKhroU0Ed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Train...\")\n",
        "%time model.fit(xtrain, ytrain)\n",
        "\n",
        "print(\"\\nPredict (Validation)...\")\n",
        "%time y_pred = model.predict(xvalid)\n",
        "predictions=[round(value) for value in y_pred]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_PwBuiWU3B_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracy = accuracy_score(yvalid, predictions)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(yvalid, y_pred))\n",
        "print(\"RMSE: %f\" % (rmse))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-vUjPp9WuFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cv_results = xgb.cv(**params, as_pandas=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lBjPE0TVKvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"\\nPredict (Test)...\")\n",
        "\n",
        "y_pred=model.predict(t_df.iloc[:,0:41])\n",
        "predictions=[round(value) for value in y_pred]\n",
        "accuracy = accuracy_score(t_df.iloc[:,41], predictions)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quEfUatvVc_6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HhiqerF1Ayu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}